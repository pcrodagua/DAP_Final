{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.optimize as opt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"ticks\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_data = pd.read_csv('../data/clean/bcdr_d0G_medc.csv')\n",
    "fs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = fs_data[\n",
    "    ['mammography_nodule', 'i_mean',     'i_skewness', 's_x_center_mass',\n",
    "    's_y_center_mass',    's_solidity', 's_extent',   't_corr','t_homo', 't_senth']]\n",
    "y_values = fs_data[\"diagnosis\"]\n",
    "y_values.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 1234\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred=y_pred)\n",
    "    sns.heatmap(cm, annot=True)\n",
    "    target_names = ['0', '1']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    print('Accuracy: {0}'.format(accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "def plot_roc(y_test, y_pred, model):\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.', label = 'AUC: %0.2f' % auc, color= 'green')\n",
    "    plt.title('{0} (ROC + AUC)'.format(model))\n",
    "    plt.xlabel('False Positive Rate (x)')\n",
    "    plt.ylabel('True Positive Rate (y)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (No Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(x_train, y_train)\n",
    "y_pred = lr_model.predict(x_test)\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(y_test, y_pred, 'Logistic Regression - NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Normalized Z-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from scipy.stats import zscore\n",
    "fs_data_n = fs_data.drop(columns=[\"diagnosis\"]).apply(zscore)\n",
    "fs_data_n['diagnosis'] = fs_data['diagnosis']\n",
    "fs_data_n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_values = fs_data_n[\n",
    "    ['mammography_nodule', 'i_mean',     'i_skewness', 's_x_center_mass',\n",
    "    's_y_center_mass',    's_solidity', 's_extent',   't_corr','t_homo', 't_senth']]\n",
    "ny_values = fs_data_n[\"diagnosis\"]\n",
    "nx_values = fs_data_n.drop(columns=['diagnosis']) \n",
    "ny_values = fs_data_n['diagnosis']\n",
    "nx_train, nx_test, ny_train, ny_test = train_test_split(nx_values, ny_values, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(nx_train, ny_train)\n",
    "ny_pred = lr_model.predict(nx_test)\n",
    "print_metrics(ny_test, ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(ny_test, ny_pred, 'Logistic Regression(Normalized)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves w/Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (Logistic Regression NN)\"\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=0)\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "plot_learning_curve(estimator, title, x_values, y_values, ylim=(0.7, 1.01), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=0)\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "plot_learning_curve(estimator, title, nx_values, ny_values, ylim=(0.7, 1.01), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polinomial Degree curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(train_error, test_error):\n",
    "    plt.plot(np.arange(0,train_error.shape[0]-1), train_error[1:], label = 'train error')\n",
    "    plt.plot(np.arange(0,train_error.shape[0]-1),test_error[1:], label = 'test error', color= 'green')\n",
    "    plt.title('Training Set')\n",
    "    plt.xlabel('Polynomial Complexity (x)')\n",
    "    plt.ylabel('MSE (y)')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "    \n",
    "def obtain_error(x_train, y_train, x_test, y_test):\n",
    "    m, n = x_train.shape\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "    hyp_model = None\n",
    "    cls_ = LogisticRegression().fit(x_train, y_train)\n",
    "    hyph_train = cls_.predict(x_train)\n",
    "    hyph_test = cls_.predict(x_test)\n",
    "    train_error = mean_squared_error(y_train, hyph_train)\n",
    "    test_error  = mean_squared_error(y_test, hyph_test)\n",
    "    return train_error, test_error\n",
    "\n",
    "\n",
    "def create_curve(fs_data, seed):\n",
    "    train, test = train_test_split(fs_data, test_size=0.3, random_state=seed)\n",
    "    \n",
    "    nx_train = train.iloc[:,:-1].values\n",
    "    ny_train = train.iloc[:,-1].values\n",
    "    # ---- \n",
    "    nx_test = test.iloc[:,:-1].values\n",
    "    ny_test = test.iloc[:,-1].values\n",
    "    # ------\n",
    "    train_error = np.zeros(nx_train.shape[1])\n",
    "    test_error = np.zeros(nx_train.shape[1])\n",
    "\n",
    "    n = nx_train.shape[1]\n",
    "    for i in range(1, n):\n",
    "        train_error[i], test_error[i] = obtain_error(nx_train[:,0:i], ny_train, nx_test[:,0:i], ny_test)\n",
    "    plot_curve(train_error, test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = fs_data[\n",
    "    ['mammography_nodule', 'i_mean',     'i_skewness', 's_x_center_mass',\n",
    "    's_y_center_mass',    's_solidity', 's_extent',   't_corr','t_homo', 't_senth',\n",
    "    'diagnosis']]\n",
    "create_curve(new_data, seed)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnew_data = fs_data_n[\n",
    "    ['mammography_nodule', 'i_mean',     'i_skewness', 's_x_center_mass',\n",
    "    's_y_center_mass',    's_solidity', 's_extent',   't_corr','t_homo', 't_senth',\n",
    "    'diagnosis']]\n",
    "create_curve(nnew_data, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyml]",
   "language": "python",
   "name": "conda-env-pyml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
